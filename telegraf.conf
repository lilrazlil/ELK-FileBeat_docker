[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

#[global_tags]
#  user = "telegraf"

# [[inputs.mem]]

# [[inputs.cpu]]
#   ## Whether to report per-cpu stats or not
#   percpu = true
#   ## Whether to report total system cpu stats or not
#   totalcpu = true
#   ## If true, collect raw CPU time metrics
#   collect_cpu_time = false
#   ## If true, compute and report the sum of all non-idle CPU states
#   report_active = false

[[inputs.disk]]
  mount_points = ["/"]

#[[inputs.diskio]]
#  devices = ["vda"]

#[[inputs.swap]]


# For InfluxDB OSS 2:
[[outputs.influxdb_v2]]
  urls = ["http://10.0.0.104:8086"]
  token = "OiQXdVhvZhM-vNl2H6Vq11a-ddiRj8krYbhODitgk4GBtSp4iVpgQIF_5ONT_Zt7Bp0pqTyLnQYMZHfPv0ctpg=="
  organization = "library"
  bucket = "library"

#[[inputs.nginx]]
#  urls = ["http://nginx/nginx_status"]

[[inputs.nginx]]
    urls = ["http://10.0.0.104/nginx_status"]
    response_timeout = "5s"
[[inputs.tail]]
   name_override = "nginxlog"
   files = ["/nginx/access.log"]
   from_beginning = true
   pipe = false
   data_format = "grok"
   grok_patterns = ["%{COMBINED_LOG_FORMAT}"]

# HTTP/HTTPS request given an address a method and a timeout
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8001/catalog-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8002/event-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8003/library-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8004/copy-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8005/delivery-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8006/reservation-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8007/information-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
[[inputs.http_response]]
  urls = ["http://10.0.0.104:8008/migration-service/swagger-ui/"]
  method = "GET"
  follow_redirects = true
  response_string_match = "Select a definition"
